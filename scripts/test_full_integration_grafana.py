"""
üéØ TEST INT√âGRATION COMPL√àTE GRAFANA
Validation pipeline: G√©n√©rateur IoT ‚Üí Prometheus ‚Üí Grafana
127 capteurs temps r√©el avec m√©triques RNCP 39394

Compatible RNCP 39394 - Expert en Syst√®mes d'Information et S√©curit√©
Auteur: Expert DevSecOps & IA Explicable
Version: 1.0.0
"""

import asyncio
import time
import json
import requests
import logging
import subprocess
import psutil
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
import pandas as pd
import numpy as np
from dataclasses import dataclass, asdict

# Configuration logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class IntegrationTestResult:
    """R√©sultat test int√©gration"""
    component: str
    test_name: str
    status: str  # PASS, FAIL, WARNING
    details: str
    duration_seconds: float
    metrics_collected: int = 0

class GrafanaIntegrationTester:
    """
    Testeur int√©gration compl√®te pipeline IoT ‚Üí Prometheus ‚Üí Grafana
    Validation temps r√©el des 127 capteurs Station Traffey√®re
    """
    
    def __init__(self):
        self.test_results: List[IntegrationTestResult] = []
        self.prometheus_url = "http://localhost:9090"
        self.grafana_url = "http://localhost:3000"
        self.iot_generator_url = "http://localhost:8090"
        self.edge_ai_url = "http://localhost:8091"
        
        # M√©triques critiques RNCP 39394
        self.critical_metrics = [
            'station_traffeyere_ph_current',
            'station_traffeyere_o2_dissous_current',
            'station_traffeyere_turbidite_current', 
            'station_traffeyere_debit_current',
            'station_traffeyere_total_sensors',
            'station_traffeyere_anomalies_total',
            'station_traffeyere_compliance_score',
            'edge_ai_prediction_latency_seconds',
            'edge_ai_anomalies_detected_total',
            'edge_ai_model_accuracy'
        ]
        
        # Configuration dashboard Grafana
        self.dashboard_panels = [
            'Station Overview',
            'pH Monitoring', 
            'Oxygen Levels',
            'Turbidity Analysis',
            'Flow Rates',
            'Temperature Monitoring',
            'Anomaly Detection',
            'AI Performance',
            'Compliance DERU',
            'Process Efficiency',
            'Alert Status'
        ]
        
        logger.info("üß™ Testeur int√©gration Grafana initialis√©")

    def record_test_result(self, component: str, test_name: str, 
                          status: str, details: str, duration: float, 
                          metrics_count: int = 0):
        """Enregistrement r√©sultat test"""
        result = IntegrationTestResult(
            component=component,
            test_name=test_name,
            status=status,
            details=details,
            duration_seconds=duration,
            metrics_collected=metrics_count
        )
        self.test_results.append(result)
        
        status_icon = "‚úÖ" if status == "PASS" else "‚ùå" if status == "FAIL" else "‚ö†Ô∏è"
        logger.info(f"{status_icon} [{component}] {test_name}: {status} - {details}")

    async def test_iot_generator_running(self) -> bool:
        """Test 1: G√©n√©rateur IoT actif et exposant m√©triques"""
        start_time = time.perf_counter()
        
        try:
            # V√©rification endpoint m√©triques
            response = requests.get(f"{self.iot_generator_url}/metrics", timeout=5)
            
            if response.status_code == 200:
                metrics_content = response.text
                metrics_count = len([line for line in metrics_content.split('\n') 
                                   if line.startswith('station_traffeyere_')])
                
                if metrics_count >= 10:  # Au moins 10 m√©triques diff√©rentes
                    self.record_test_result(
                        "IoT Generator", 
                        "G√©n√©rateur actif et m√©triques expos√©es",
                        "PASS",
                        f"{metrics_count} m√©triques disponibles",
                        time.perf_counter() - start_time,
                        metrics_count
                    )
                    return True
                else:
                    self.record_test_result(
                        "IoT Generator",
                        "M√©triques insuffisantes",
                        "WARNING", 
                        f"Seulement {metrics_count} m√©triques trouv√©es",
                        time.perf_counter() - start_time
                    )
                    return False
            else:
                self.record_test_result(
                    "IoT Generator",
                    "Endpoint m√©triques inaccessible",
                    "FAIL",
                    f"HTTP {response.status_code}",
                    time.perf_counter() - start_time
                )
                return False
                
        except Exception as e:
            self.record_test_result(
                "IoT Generator",
                "Connexion g√©n√©rateur IoT",
                "FAIL",
                f"Erreur: {str(e)}",
                time.perf_counter() - start_time
            )
            return False

    async def test_edge_ai_engine_running(self) -> bool:
        """Test 2: Edge AI Engine actif et fonctionnel"""
        start_time = time.perf_counter()
        
        try:
            response = requests.get(f"{self.edge_ai_url}/metrics", timeout=5)
            
            if response.status_code == 200:
                metrics_content = response.text
                ai_metrics = [line for line in metrics_content.split('\n') 
                            if line.startswith('edge_ai_')]
                
                if len(ai_metrics) >= 5:
                    self.record_test_result(
                        "Edge AI Engine",
                        "Moteur IA actif",
                        "PASS",
                        f"{len(ai_metrics)} m√©triques IA disponibles",
                        time.perf_counter() - start_time,
                        len(ai_metrics)
                    )
                    return True
                else:
                    self.record_test_result(
                        "Edge AI Engine",
                        "M√©triques IA insuffisantes", 
                        "WARNING",
                        f"Seulement {len(ai_metrics)} m√©triques IA",
                        time.perf_counter() - start_time
                    )
                    return False
            else:
                self.record_test_result(
                    "Edge AI Engine",
                    "Endpoint IA inaccessible",
                    "FAIL",
                    f"HTTP {response.status_code}",
                    time.perf_counter() - start_time
                )
                return False
                
        except Exception as e:
            self.record_test_result(
                "Edge AI Engine", 
                "Connexion Edge AI",
                "FAIL",
                f"Erreur: {str(e)}",
                time.perf_counter() - start_time
            )
            return False

    async def test_prometheus_scraping(self) -> bool:
        """Test 3: Prometheus scraping m√©triques IoT"""
        start_time = time.perf_counter()
        
        try:
            # Test requ√™te m√©triques critiques
            critical_found = 0
            total_metrics_scraped = 0
            
            for metric in self.critical_metrics:
                query_url = f"{self.prometheus_url}/api/v1/query"
                params = {'query': metric}
                
                response = requests.get(query_url, params=params, timeout=10)
                
                if response.status_code == 200:
                    data = response.json()
                    if data.get('status') == 'success' and data.get('data', {}).get('result'):
                        critical_found += 1
                        total_metrics_scraped += len(data['data']['result'])
            
            # V√©rification taux succ√®s
            success_rate = (critical_found / len(self.critical_metrics)) * 100
            
            if success_rate >= 80:  # 80% m√©triques critiques minimum
                self.record_test_result(
                    "Prometheus",
                    "Scraping m√©triques critiques", 
                    "PASS",
                    f"{critical_found}/{len(self.critical_metrics)} m√©triques ({success_rate:.1f}%)",
                    time.perf_counter() - start_time,
                    total_metrics_scraped
                )
                return True
            elif success_rate >= 50:
                self.record_test_result(
                    "Prometheus",
                    "Scraping partiel",
                    "WARNING",
                    f"{critical_found}/{len(self.critical_metrics)} m√©triques ({success_rate:.1f}%)",
                    time.perf_counter() - start_time,
                    total_metrics_scraped
                )
                return False
            else:
                self.record_test_result(
                    "Prometheus", 
                    "Scraping insuffisant",
                    "FAIL",
                    f"Seulement {critical_found}/{len(self.critical_metrics)} m√©triques",
                    time.perf_counter() - start_time
                )
                return False
                
        except Exception as e:
            self.record_test_result(
                "Prometheus",
                "Requ√™te m√©triques",
                "FAIL", 
                f"Erreur: {str(e)}",
                time.perf_counter() - start_time
            )
            return False

    async def test_grafana_connectivity(self) -> bool:
        """Test 4: Connectivit√© Grafana et datasource Prometheus"""
        start_time = time.perf_counter()
        
        try:
            # Test sant√© Grafana
            health_response = requests.get(f"{self.grafana_url}/api/health", timeout=10)
            
            if health_response.status_code != 200:
                self.record_test_result(
                    "Grafana",
                    "Sant√© serveur Grafana",
                    "FAIL",
                    f"HTTP {health_response.status_code}",
                    time.perf_counter() - start_time
                )
                return False
            
            # Test datasources (n√©cessite authentification, simulation)
            # En production, utiliser API key Grafana
            self.record_test_result(
                "Grafana",
                "Connectivit√© serveur",
                "PASS", 
                "Serveur Grafana accessible",
                time.perf_counter() - start_time
            )
            
            return True
            
        except Exception as e:
            self.record_test_result(
                "Grafana",
                "Connexion serveur",
                "FAIL",
                f"Erreur: {str(e)}",
                time.perf_counter() - start_time
            )
            return False

    async def test_real_time_data_flow(self) -> bool:
        """Test 5: Flux donn√©es temps r√©el IoT ‚Üí Prometheus"""
        start_time = time.perf_counter()
        
        try:
            # Collecte m√©triques √† T0
            initial_metrics = {}
            for metric in ['station_traffeyere_total_sensors', 'station_traffeyere_anomalies_total']:
                query_url = f"{self.prometheus_url}/api/v1/query"
                response = requests.get(query_url, params={'query': metric}, timeout=5)
                
                if response.status_code == 200:
                    data = response.json()
                    if data.get('data', {}).get('result'):
                        initial_metrics[metric] = float(data['data']['result'][0]['value'][1])
            
            # Attente 10 secondes pour nouveau cycle
            await asyncio.sleep(10)
            
            # Collecte m√©triques √† T+10s
            updated_metrics = {}
            for metric in initial_metrics.keys():
                query_url = f"{self.prometheus_url}/api/v1/query"
                response = requests.get(query_url, params={'query': metric}, timeout=5)
                
                if response.status_code == 200:
                    data = response.json()
                    if data.get('data', {}).get('result'):
                        updated_metrics[metric] = float(data['data']['result'][0]['value'][1])
            
            # V√©rification mise √† jour
            updates_detected = 0
            for metric in initial_metrics.keys():
                if metric in updated_metrics:
                    if updated_metrics[metric] != initial_metrics[metric]:
                        updates_detected += 1
            
            if updates_detected >= 1:
                self.record_test_result(
                    "Data Flow",
                    "Flux temps r√©el actif",
                    "PASS",
                    f"{updates_detected} m√©triques mises √† jour",
                    time.perf_counter() - start_time
                )
                return True
            else:
                self.record_test_result(
                    "Data Flow",
                    "Pas de mise √† jour d√©tect√©e",
                    "WARNING",
                    "M√©triques statiques sur 10s",
                    time.perf_counter() - start_time
                )
                return False
                
        except Exception as e:
            self.record_test_result(
                "Data Flow",
                "Test flux temps r√©el",
                "FAIL",
                f"Erreur: {str(e)}",
                time.perf_counter() - start_time
            )
            return False

    async def test_anomaly_detection_pipeline(self) -> bool:
        """Test 6: Pipeline d√©tection anomalies Edge AI"""
        start_time = time.perf_counter()
        
        try:
            # V√©rification m√©triques anomalies Edge AI
            query_url = f"{self.prometheus_url}/api/v1/query"
            
            # M√©triques cl√©s d√©tection anomalies
            anomaly_metrics = [
                'edge_ai_anomalies_detected_total',
                'edge_ai_prediction_latency_seconds', 
                'edge_ai_predictions_total'
            ]
            
            metrics_found = 0
            latency_compliant = False
            
            for metric in anomaly_metrics:
                response = requests.get(query_url, params={'query': metric}, timeout=5)
                
                if response.status_code == 200:
                    data = response.json()
                    if data.get('data', {}).get('result'):
                        metrics_found += 1
                        
                        # V√©rification latence P95 < 0.28ms
                        if metric == 'edge_ai_prediction_latency_seconds':
                            # Query percentile 95
                            p95_query = f'histogram_quantile(0.95, {metric})'
                            p95_response = requests.get(query_url, params={'query': p95_query}, timeout=5)
                            
                            if p95_response.status_code == 200:
                                p95_data = p95_response.json()
                                if p95_data.get('data', {}).get('result'):
                                    p95_value = float(p95_data['data']['result'][0]['value'][1])
                                    latency_compliant = p95_value < 0.00028  # 0.28ms en secondes
            
            if metrics_found >= 2:
                compliance_status = "‚úÖ Conforme RNCP" if latency_compliant else "‚ö†Ô∏è Latence √©lev√©e"
                self.record_test_result(
                    "Anomaly Detection",
                    "Pipeline d√©tection anomalies",
                    "PASS" if latency_compliant else "WARNING",
                    f"{metrics_found}/3 m√©triques - {compliance_status}",
                    time.perf_counter() - start_time,
                    metrics_found
                )
                return True
            else:
                self.record_test_result(
                    "Anomaly Detection", 
                    "M√©triques anomalies manquantes",
                    "FAIL",
                    f"Seulement {metrics_found}/3 m√©triques d√©tection",
                    time.perf_counter() - start_time
                )
                return False
                
        except Exception as e:
            self.record_test_result(
                "Anomaly Detection",
                "Test pipeline anomalies",
                "FAIL",
                f"Erreur: {str(e)}",
                time.perf_counter() - start_time
            )
            return False

    async def test_dashboard_panel_queries(self) -> bool:
        """Test 7: Validation requ√™tes dashboard Grafana (simulation)"""
        start_time = time.perf_counter()
        
        try:
            # Simulation requ√™tes typiques dashboard Grafana
            dashboard_queries = [
                'station_traffeyere_ph_current',
                'rate(station_traffeyere_anomalies_total[5m])',
                'avg(station_traffeyere_treatment_efficiency_percent)',
                'histogram_quantile(0.95, edge_ai_prediction_latency_seconds)',
                'station_traffeyere_compliance_score'
            ]
            
            successful_queries = 0
            query_url = f"{self.prometheus_url}/api/v1/query"
            
            for query in dashboard_queries:
                try:
                    response = requests.get(query_url, params={'query': query}, timeout=5)
                    
                    if response.status_code == 200:
                        data = response.json()
                        if data.get('status') == 'success':
                            successful_queries += 1
                            
                except requests.RequestException:
                    continue
            
            success_rate = (successful_queries / len(dashboard_queries)) * 100
            
            if success_rate >= 80:
                self.record_test_result(
                    "Dashboard Queries",
                    "Requ√™tes dashboard valides",
                    "PASS",
                    f"{successful_queries}/{len(dashboard_queries)} requ√™tes OK ({success_rate:.1f}%)",
                    time.perf_counter() - start_time,
                    successful_queries
                )
                return True
            else:
                self.record_test_result(
                    "Dashboard Queries",
                    "Requ√™tes partiellement valides", 
                    "WARNING",
                    f"{successful_queries}/{len(dashboard_queries)} requ√™tes OK ({success_rate:.1f}%)",
                    time.perf_counter() - start_time,
                    successful_queries
                )
                return False
                
        except Exception as e:
            self.record_test_result(
                "Dashboard Queries",
                "Validation requ√™tes",
                "FAIL",
                f"Erreur: {str(e)}",
                time.perf_counter() - start_time
            )
            return False

    async def test_rncp_compliance_metrics(self) -> bool:
        """Test 8: M√©triques conformit√© RNCP 39394"""
        start_time = time.perf_counter()
        
        try:
            # M√©triques critiques RNCP 39394
            rncp_metrics = {
                'station_traffeyere_compliance_score': {'min': 85.0, 'target': 95.0},
                'edge_ai_model_accuracy': {'min': 0.85, 'target': 0.95},
                'station_traffeyere_treatment_efficiency_percent': {'min': 85.0, 'target': 92.0}
            }
            
            compliant_metrics = 0
            query_url = f"{self.prometheus_url}/api/v1/query"
            
            compliance_details = []
            
            for metric, thresholds in rncp_metrics.items():
                try:
                    response = requests.get(query_url, params={'query': metric}, timeout=5)
                    
                    if response.status_code == 200:
                        data = response.json()
                        if data.get('data', {}).get('result'):
                            value = float(data['data']['result'][0]['value'][1])
                            
                            if value >= thresholds['target']:
                                compliant_metrics += 1
                                compliance_details.append(f"{metric}: {value:.1f} ‚úÖ")
                            elif value >= thresholds['min']:
                                compliance_details.append(f"{metric}: {value:.1f} ‚ö†Ô∏è")
                            else:
                                compliance_details.append(f"{metric}: {value:.1f} ‚ùå")
                        else:
                            compliance_details.append(f"{metric}: Donn√©es manquantes")
                    else:
                        compliance_details.append(f"{metric}: Inaccessible")
                        
                except Exception:
                    compliance_details.append(f"{metric}: Erreur requ√™te")
            
            compliance_rate = (compliant_metrics / len(rncp_metrics)) * 100
            
            if compliance_rate >= 80:
                self.record_test_result(
                    "RNCP Compliance",
                    "Conformit√© RNCP 39394",
                    "PASS",
                    f"{compliant_metrics}/{len(rncp_metrics)} m√©triques conformes ({compliance_rate:.1f}%)",
                    time.perf_counter() - start_time,
                    compliant_metrics
                )
                return True
            else:
                self.record_test_result(
                    "RNCP Compliance",
                    "Conformit√© partielle",
                    "WARNING",
                    f"Seulement {compliant_metrics}/{len(rncp_metrics)} conformes",
                    time.perf_counter() - start_time,
                    compliant_metrics
                )
                return False
                
        except Exception as e:
            self.record_test_result(
                "RNCP Compliance",
                "Test conformit√© RNCP",
                "FAIL",
                f"Erreur: {str(e)}",
                time.perf_counter() - start_time
            )
            return False

    async def run_full_integration_tests(self) -> Dict[str, Any]:
        """Ex√©cution suite compl√®te tests int√©gration"""
        logger.info("üéØ D√©marrage tests int√©gration compl√®te Grafana")
        test_start = time.perf_counter()
        
        # Ex√©cution s√©quentielle tous tests
        tests = [
            self.test_iot_generator_running(),
            self.test_edge_ai_engine_running(), 
            self.test_prometheus_scraping(),
            self.test_grafana_connectivity(),
            self.test_real_time_data_flow(),
            self.test_anomaly_detection_pipeline(),
            self.test_dashboard_panel_queries(),
            self.test_rncp_compliance_metrics()
        ]
        
        # Attente r√©sultats
        await asyncio.gather(*tests)
        
        total_duration = time.perf_counter() - test_start
        
        # Calcul statistiques
        passed_tests = len([r for r in self.test_results if r.status == "PASS"])
        warning_tests = len([r for r in self.test_results if r.status == "WARNING"])
        failed_tests = len([r for r in self.test_results if r.status == "FAIL"])
        total_tests = len(self.test_results)
        
        total_metrics = sum(r.metrics_collected for r in self.test_results)
        
        success_rate = (passed_tests / total_tests) * 100 if total_tests > 0 else 0
        
        # R√©sultats finaux
        results = {
            'total_tests': total_tests,
            'passed': passed_tests,
            'warnings': warning_tests,
            'failed': failed_tests,
            'success_rate_percent': success_rate,
            'total_duration_seconds': total_duration,
            'total_metrics_validated': total_metrics,
            'test_results': [asdict(r) for r in self.test_results]
        }
        
        return results

    def print_test_summary(self, results: Dict[str, Any]):
        """Affichage r√©sum√© tests"""
        print("\n" + "="*70)
        print("üéØ RAPPORT TESTS INT√âGRATION GRAFANA - STATION TRAFFEY√àRE")
        print("="*70)
        print(f"üìä Total tests ex√©cut√©s: {results['total_tests']}")
        print(f"‚úÖ Tests r√©ussis: {results['passed']}")
        print(f"‚ö†Ô∏è  Tests avec warnings: {results['warnings']}")
        print(f"‚ùå Tests √©chou√©s: {results['failed']}")
        print(f"üéØ Taux de succ√®s: {results['success_rate_percent']:.1f}%")
        print(f"‚è±Ô∏è  Dur√©e totale: {results['total_duration_seconds']:.2f}s")
        print(f"üìà M√©triques valid√©es: {results['total_metrics_validated']}")
        
        print("\n" + "="*70)
        print("üìã D√âTAIL DES TESTS")
        print("="*70)
        
        for result in self.test_results:
            status_icon = "‚úÖ" if result.status == "PASS" else "‚ùå" if result.status == "FAIL" else "‚ö†Ô∏è"
            print(f"{status_icon} [{result.component}] {result.test_name}")
            print(f"   Status: {result.status}")
            print(f"   D√©tails: {result.details}")
            print(f"   Dur√©e: {result.duration_seconds:.2f}s")
            if result.metrics_collected > 0:
                print(f"   M√©triques: {result.metrics_collected}")
            print()
        
        # √âvaluation conformit√© globale
        print("="*70)
        if results['success_rate_percent'] >= 90:
            print("üèÜ INT√âGRATION GRAFANA - EXCELLENT")
            print("‚úÖ Pipeline IoT ‚Üí Prometheus ‚Üí Grafana pleinement op√©rationnel")
            print("‚úÖ Conforme aux exigences RNCP 39394")
        elif results['success_rate_percent'] >= 75:
            print("üéØ INT√âGRATION GRAFANA - SATISFAISANT") 
            print("‚úÖ Pipeline principal fonctionnel")
            print("‚ö†Ô∏è  Optimisations mineures recommand√©es")
        elif results['success_rate_percent'] >= 50:
            print("‚ö†Ô∏è INT√âGRATION GRAFANA - PROBL√àMES D√âTECT√âS")
            print("‚ùå Plusieurs composants n√©cessitent attention")
            print("üîß Corrections requises avant production")
        else:
            print("‚ùå INT√âGRATION GRAFANA - CRITIQUE")
            print("‚ùå Pipeline non fonctionnel")
            print("üö® Intervention urgente requise")
        
        print("="*70)

async def main():
    """Fonction principale test int√©gration Grafana"""
    print("üß™ TEST INT√âGRATION COMPL√àTE - PIPELINE GRAFANA")
    print("=" * 65)
    print("Validation: G√©n√©rateur IoT ‚Üí Prometheus ‚Üí Grafana")
    print("127 capteurs Station Traffey√®re en temps r√©el")
    print("M√©triques critiques RNCP 39394")
    print("=" * 65)
    
    # Initialisation testeur
    tester = GrafanaIntegrationTester()
    
    # Information pr√©liminaire
    print(f"\nüéØ Configuration de test:")
    print(f"   ‚Ä¢ G√©n√©rateur IoT: {tester.iot_generator_url}")
    print(f"   ‚Ä¢ Edge AI Engine: {tester.edge_ai_url}")
    print(f"   ‚Ä¢ Prometheus: {tester.prometheus_url}")
    print(f"   ‚Ä¢ Grafana: {tester.grafana_url}")
    print(f"   ‚Ä¢ M√©triques critiques: {len(tester.critical_metrics)}")
    
    # Message instruction
    print(f"\nüìã Instructions:")
    print(f"   1. D√©marrer le g√©n√©rateur IoT: python scripts/iot_data_generator.py")
    print(f"   2. D√©marrer Edge AI Engine: python scripts/edge_ai_engine.py")
    print(f"   3. S'assurer que Prometheus et Grafana sont actifs")
    print(f"   4. Ce script validera l'int√©gration compl√®te")
    
    input(f"\n‚è≥ Appuyez sur Entr√©e quand tous les services sont d√©marr√©s...")
    
    # Ex√©cution tests
    print(f"\nüöÄ D√©marrage tests int√©gration...")
    results = await tester.run_full_integration_tests()
    
    # Affichage r√©sultats
    tester.print_test_summary(results)
    
    # Export JSON optionnel
    output_file = f"integration_test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"üíæ R√©sultats export√©s: {output_file}")

if __name__ == "__main__":
    asyncio.run(main())
