import { useState, useCallback, useRef, useEffect } from 'react';
import axios from 'axios';
import { ConversationMessage, AIProvider, ConversationContext, AIResponse } from '@types/conversation.types';

interface UseConversationalAIOptions {
  provider: AIProvider;
  expertiseLevel: 'novice' | 'intermediate' | 'expert' | 'researcher';
  context?: ConversationContext;
  maxTokens?: number;
  temperature?: number;
}

interface AIProviderConfig {
  baseURL: string;
  model: string;
  apiKey?: string;
  headers: Record<string, string>;
  formatMessage: (message: string, context: any) => any;
  parseResponse: (response: any) => AIResponse;
}

export const useConversationalAI = (options: UseConversationalAIOptions) => {
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState<Error | null>(null);
  const [conversation, setConversation] = useState<ConversationMessage[]>([]);
  const [currentProvider, setCurrentProvider] = useState<AIProvider>(options.provider);
  const [contextData, setContextData] = useState<any>(options.context);
  
  const conversationHistoryRef = useRef<ConversationMessage[]>([]);
  const abortControllerRef = useRef<AbortController | null>(null);

  // Configuration des providers IA
  const getProviderConfig = (provider: AIProvider): AIProviderConfig => {
    const configs: Record<AIProvider, AIProviderConfig> = {
      gemini: {
        baseURL: process.env.REACT_APP_GEMINI_API_URL || 'https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent',
        model: 'gemini-pro',
        headers: {
          'Content-Type': 'application/json',
        },
        formatMessage: (message: string, context: any) => ({
          contents: [{
            parts: [{
              text: `Tu es un assistant IA expert pour la Station Traffey√®re, une station d'√©puration d'eau avec 127 capteurs IoT et un syst√®me d'IA Edge.

Contexte technique actuel:
- Capteurs actifs: ${context?.activeSensors || 0}/${context?.currentSensors || 127}
- Anomalies d√©tect√©es: ${context?.anomalies || 0}
- Performance IA Edge: ${context?.metrics?.edgeAI?.averageLatency || 0.28}ms
- Niveau utilisateur: ${options.expertiseLevel}
- Timestamp: ${context?.timestamp || new Date().toISOString()}

${context?.lastExplanation ? `
Derni√®re explication XAI:
- Confiance: ${Math.round((context.lastExplanation.confidence || 0) * 100)}%
- M√©thode: ${context.lastExplanation.method || 'SHAP'}
- Temps traitement: ${context.lastExplanation.processingTime || 0}ms
` : ''}

Consignes:
- R√©ponds en fran√ßais clair et pr√©cis
- Adapte ton niveau selon l'expertise: ${options.expertiseLevel}
- Concentre-toi sur les aspects IoT, IA et monitoring
- Propose des actions concr√®tes quand pertinent
- Reste dans le contexte de la station d'√©puration

Question utilisateur: ${message}`
            }]
          }],
          generationConfig: {
            temperature: options.temperature || 0.7,
            candidateCount: 1,
            maxOutputTokens: options.maxTokens || 1000,
          }
        }),
        parseResponse: (response: any): AIResponse => ({
          text: response.candidates?.[0]?.content?.parts?.[0]?.text || 'R√©ponse non disponible',
          confidence: 0.9, // Gemini ne fournit pas de score de confiance explicite
          provider: 'gemini',
          context: response.candidates?.[0]?.safetyRatings,
          suggestions: [],
          actions: extractActions(response.candidates?.[0]?.content?.parts?.[0]?.text || ''),
        }),
      },

      claude: {
        baseURL: process.env.REACT_APP_CLAUDE_API_URL || 'https://api.anthropic.com/v1/messages',
        model: 'claude-3-sonnet-20240229',
        headers: {
          'Content-Type': 'application/json',
          'x-api-key': process.env.REACT_APP_CLAUDE_API_KEY || '',
          'anthropic-version': '2023-06-01',
        },
        formatMessage: (message: string, context: any) => ({
          model: 'claude-3-sonnet-20240229',
          max_tokens: options.maxTokens || 1000,
          temperature: options.temperature || 0.7,
          system: `Tu es Claude, assistant IA expert pour la Station Traffey√®re IoT/IA. 

Tu disposes des donn√©es temps r√©el:
- ${context?.activeSensors || 0} capteurs actifs sur ${context?.currentSensors || 127}
- ${context?.anomalies || 0} anomalies d√©tect√©es
- Performance IA: ${context?.metrics?.edgeAI?.averageLatency || 0.28}ms
- Utilisateur niveau: ${options.expertiseLevel}

Ton r√¥le: analyser, expliquer et conseiller sur l'infrastructure IoT avec pr√©cision technique adapt√©e au niveau utilisateur.`,
          messages: [{
            role: 'user',
            content: message
          }]
        }),
        parseResponse: (response: any): AIResponse => ({
          text: response.content?.[0]?.text || 'R√©ponse non disponible',
          confidence: 0.95, // Claude est g√©n√©ralement tr√®s fiable
          provider: 'claude',
          context: response.usage,
          suggestions: [],
          actions: extractActions(response.content?.[0]?.text || ''),
        }),
      },

      gpt4: {
        baseURL: process.env.REACT_APP_OPENAI_API_URL || 'https://api.openai.com/v1/chat/completions',
        model: 'gpt-4-turbo-preview',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${process.env.REACT_APP_OPENAI_API_KEY || ''}`,
        },
        formatMessage: (message: string, context: any) => ({
          model: 'gpt-4-turbo-preview',
          messages: [
            {
              role: 'system',
              content: `Tu es un assistant IA GPT-4 expert pour la Station Traffey√®re, infrastructure critique IoT/IA.

√âtat syst√®me temps r√©el:
- Capteurs: ${context?.activeSensors || 0}/${context?.currentSensors || 127} actifs
- Anomalies: ${context?.anomalies || 0}
- Latence IA: ${context?.metrics?.edgeAI?.averageLatency || 0.28}ms (objectif <1ms)
- Niveau utilisateur: ${options.expertiseLevel}

Mission: assistance experte IoT/IA avec explications adapt√©es au niveau technique de l'utilisateur. Focus sur analyse de donn√©es, d√©tection d'anomalies, et optimisation performance.`
            },
            {
              role: 'user',
              content: message
            }
          ],
          temperature: options.temperature || 0.7,
          max_tokens: options.maxTokens || 1000,
          top_p: 1,
          frequency_penalty: 0,
          presence_penalty: 0,
        }),
        parseResponse: (response: any): AIResponse => ({
          text: response.choices?.[0]?.message?.content || 'R√©ponse non disponible',
          confidence: response.choices?.[0]?.finish_reason === 'stop' ? 0.9 : 0.7,
          provider: 'gpt4',
          context: response.usage,
          suggestions: [],
          actions: extractActions(response.choices?.[0]?.message?.content || ''),
        }),
      },

      perplexity: {
        baseURL: process.env.REACT_APP_PERPLEXITY_API_URL || 'https://api.perplexity.ai/chat/completions',
        model: 'llama-3.1-sonar-large-128k-online',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${process.env.REACT_APP_PERPLEXITY_API_KEY || ''}',
        },
        formatMessage: (message: string, context: any) => ({
          model: 'llama-3.1-sonar-large-128k-online',
          messages: [
            {
              role: 'system',
              content: `Assistant IA Perplexity pour Station Traffey√®re IoT/IA. Combine recherche temps r√©el + analyse donn√©es syst√®me.

Donn√©es actuelles:
- Infrastructure: ${context?.currentSensors || 127} capteurs IoT
- √âtat: ${context?.activeSensors || 0} actifs, ${context?.anomalies || 0} anomalies
- Performance: ${context?.metrics?.edgeAI?.averageLatency || 0.28}ms latence IA
- Utilisateur: ${options.expertiseLevel}

Capacit√©s: recherche info r√©centes + analyse contexte technique + recommandations expertes.`
            },
            {
              role: 'user',
              content: message
            }
          ],
          temperature: options.temperature || 0.2, // Plus faible pour Perplexity
          max_tokens: options.maxTokens || 1000,
          top_p: 0.9,
          search_domain_filter: ['technical', 'industrial', 'iot'],
          return_citations: true,
        }),
        parseResponse: (response: any): AIResponse => ({
          text: response.choices?.[0]?.message?.content || 'R√©ponse non disponible',
          confidence: response.choices?.[0]?.finish_reason === 'stop' ? 0.85 : 0.7,
          provider: 'perplexity',
          context: response.usage,
          suggestions: [],
          actions: extractActions(response.choices?.[0]?.message?.content || ''),
          citations: response.citations, // Sp√©cifique √† Perplexity
        }),
      },
    };

    return configs[provider];
  };

  // Extraction d'actions sugg√©r√©es √† partir du texte
  const extractActions = (text: string): string[] => {
    const actions: string[] = [];
    
    // Patterns pour d√©tecter les actions sugg√©r√©es
    const actionPatterns = [
      /(?:vous pouvez|je recommande|il serait bien de|essayez de)\s+(.+?)(?:\.|,|$)/gi,
      /(?:action|√©tape|proc√©dure)\s*:\s*(.+?)(?:\.|,|$)/gi,
      /(?:v√©rifiez|contr√¥lez|analysez|examinez)\s+(.+?)(?:\.|,|$)/gi,
    ];

    actionPatterns.forEach(pattern => {
      let match;
      while ((match = pattern.exec(text)) !== null) {
        const action = match[1]?.trim();
        if (action && action.length > 10 && action.length < 100) {
          actions.push(action);
        }
      }
    });

    return actions.slice(0, 3); // Limite √† 3 actions
  };

  // Fonction d'envoi de message
  const sendMessage = useCallback(async (message: string, context?: any): Promise<AIResponse> => {
    if (abortControllerRef.current) {
      abortControllerRef.current.abort();
    }

    abortControllerRef.current = new AbortController();
    setLoading(true);
    setError(null);

    try {
      const config = getProviderConfig(currentProvider);
      const enrichedContext = { ...contextData, ...context };
      
      // Pr√©paration de la requ√™te
      const requestData = config.formatMessage(message, enrichedContext);
      
      // Ajout de l'API key si n√©cessaire
      let headers = config.headers;
      if (currentProvider === 'gemini' && process.env.REACT_APP_GEMINI_API_KEY) {
        headers = {
          ...headers,
          'x-goog-api-key': process.env.REACT_APP_GEMINI_API_KEY,
        };
      }

      console.log(`üì§ Envoi vers ${currentProvider.toUpperCase()}:`, {
        provider: currentProvider,
        message: message.substring(0, 100) + '...',
        context: {
          sensors: enrichedContext?.currentSensors,
          anomalies: enrichedContext?.anomalies,
          expertiseLevel: options.expertiseLevel,
        }
      });

      // Appel API avec gestion d'erreur et timeout
      const response = await axios({
        method: 'POST',
        url: config.baseURL,
        headers,
        data: requestData,
        timeout: 30000, // 30 secondes
        signal: abortControllerRef.current.signal,
      });

      console.log(`üì• R√©ponse ${currentProvider.toUpperCase()}:`, {
        status: response.status,
        dataKeys: Object.keys(response.data || {}),
      });

      // Parse de la r√©ponse selon le provider
      const parsedResponse = config.parseResponse(response.data);
      
      // Mise √† jour de l'historique
      const userMessage: ConversationMessage = {
        id: Date.now().toString(),
        text: message,
        sender: 'user',
        timestamp: new Date(),
        language: 'fr',
      };

      const aiMessage: ConversationMessage = {
        id: (Date.now() + 1).toString(),
        text: parsedResponse.text,
        sender: 'ai',
        timestamp: new Date(),
        language: 'fr',
        confidence: parsedResponse.confidence,
        provider: currentProvider,
        context: parsedResponse.context,
        suggestions: parsedResponse.suggestions,
        actions: parsedResponse.actions,
      };

      conversationHistoryRef.current = [...conversationHistoryRef.current, userMessage, aiMessage];
      setConversation([...conversationHistoryRef.current]);

      return parsedResponse;

    } catch (error: any) {
      console.error(`‚ùå Erreur ${currentProvider.toUpperCase()}:`, error);
      
      if (error.code === 'ECONNABORTED') {
        setError(new Error('Timeout: La requ√™te a pris trop de temps'));
      } else if (error.response?.status === 429) {
        setError(new Error('Trop de requ√™tes: Veuillez patienter quelques secondes'));
      } else if (error.response?.status === 401) {
        setError(new Error('Erreur d\'authentification: V√©rifiez les cl√©s API'));
      } else if (error.response?.status === 402) {
        setError(new Error('Quota d√©pass√©: V√©rifiez votre abonnement API'));
      } else {
        setError(error);
      }

      // Fallback response en cas d'erreur
      return {
        text: `D√©sol√©, je rencontre des difficult√©s techniques avec ${currentProvider}. Veuillez r√©essayer ou changer de mod√®le IA dans les param√®tres.`,
        confidence: 0,
        provider: currentProvider,
        context: { error: error.message },
        suggestions: [],
        actions: ['Changer de mod√®le IA', 'R√©essayer plus tard', 'Contacter le support'],
      };

    } finally {
      setLoading(false);
      abortControllerRef.current = null;
    }
  }, [currentProvider, contextData, options.expertiseLevel, options.maxTokens, options.temperature]);

  // Changement de provider
  const setProvider = useCallback((provider: AIProvider) => {
    setCurrentProvider(provider);
    console.log(`üîÑ Changement vers ${provider.toUpperCase()}`);
  }, []);

  // Nettoyage de la conversation
  const clearConversation = useCallback(() => {
    conversationHistoryRef.current = [];
    setConversation([]);
    console.log('üßπ Conversation effac√©e');
  }, []);

  // Mise √† jour du contexte
  const updateContext = useCallback((newContext: Partial<ConversationContext>) => {
    setContextData(prev => ({ ...prev, ...newContext }));
  }, []);

  // Annulation de la requ√™te en cours
  const cancelRequest = useCallback(() => {
    if (abortControllerRef.current) {
      abortControllerRef.current.abort();
      setLoading(false);
    }
  }, []);

  // Nettoyage au d√©montage
  useEffect(() => {
    return () => {
      if (abortControllerRef.current) {
        abortControllerRef.current.abort();
      }
    };
  }, []);

  // Validation des cl√©s API (c√¥t√© client pour debug uniquement)
  const validateAPIKeys = useCallback(() => {
    const keys = {
      gemini: !!process.env.REACT_APP_GEMINI_API_KEY,
      claude: !!process.env.REACT_APP_CLAUDE_API_KEY,
      gpt4: !!process.env.REACT_APP_OPENAI_API_KEY,
      perplexity: !!process.env.REACT_APP_PERPLEXITY_API_KEY,
    };

    console.log('üîë √âtat des cl√©s API:', keys);
    return keys;
  }, []);

  return {
    // Actions principales
    sendMessage,
    setProvider,
    clearConversation,
    updateContext,
    cancelRequest,
    validateAPIKeys,

    // √âtat
    loading,
    error,
    conversation,
    currentProvider,
    contextData,

    // Configuration
    supportedProviders: ['gemini', 'claude', 'gpt4', 'perplexity'] as AIProvider[],
    expertiseLevel: options.expertiseLevel,
  };
};

export default useConversationalAI;